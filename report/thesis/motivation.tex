\chapter{Motivation}
The communication of HPC applications in distributed memory systems with MPI is thoroughly researched in many publications, like ... or ... .
With respect to the BSP modell, GPU computing has many similarities to computing in distributed memory systems. Each CTA computes independently and information is stored in memory. CTAs in subsequent kernels read this data, creating patterns of communication. This resembles MPI, as each node computes locally, and then communicates and synchronizes with other nodes, before the next part of computation.
Yet, intra-application communication in GPUs has not been researched, despite the fact that General Purpose GPU (GPGPU) computing is a well established part of high performance computing, as many Top500 
systems \cite{Top500} use GPUs to accelarate computation. 

In GPUs, parallel hardware continues scaling with every generation, from 512 CUDA cores in Fermi  \cite{FermiPaper} to 3840 CUDA cores in the current Pascal generation \cite{PascalPaper}. This scaling is supported by CUDAs programming model, allowing performance portable applications. With increasing potential parallelism and concurrency, communication patterns, volumes and complexity also increase or change.
An understanding of intra-application communication is fundamental for the scaling of applications to multiple
GPUs.
%HPC has massively distributed memory parallel applications, which are well researched.
%GPUs are like that, too. But not researched, yet because shared memory architecture.
%Fundamentally interesting to research behaviour, when scaling to multiple GPUs.

To create this understanding, an in-depth knowledge about the communication is required. The knowledge can be generated
using different techniques.
Spafford et al. \cite{Spafford:2012:ADS:2388996.2389110} categorized these techniques and presented several 
metrics for evaluation, including speed and accuracy. For high accuracy requirements, GPGPU simulators provide cycle-accurate simulation results for an application. However, it takes very long to simulate every hardware cycle in software which makes it a time-consuming process. 
Another issue with simulation is the implementation of different hardware generation. GPGPU-Sim, for example, only supports Fermi architectures. \cite{gpgpu}

Analytical modelling uses mathematical models, trying to create an approximation on the applications behaviour.
While this can be done quickly, accuracy is usually lacking, especially for dynamic or data-depended applications.

MPI communication is characterized using traces. An application instrumented for tracing logs information on a desired metric and stores the data for later analysis. Instrumented code is faster than simulations as it runs on the actual hardware and can be used on any hardware generation. Although slower than analytical modelling, it is more accurate since it is generated while the actual application executes, using a real dataset.
These features make instrumentation and tracing the tool of choice for this thesis.


%\begin{itemize}
%	\item GPUs are not CPUs. Different design philosophy leads to different application optimizations
%	\item GPU's concurrent hardware continues to scale
%	\item therefore optimizations of data movements, consistency etc. are required to harness compute power of new hardware generations
%	\item Complex Applications have non-trivial communication patterns in data parallel kernels, which are not researched yet.
%	\item This work is aimed to generate understanding of communication patterns to help build and optimize Tools (Mekong etc.) and Applications
%	\item Generate Traces with compiler instrumentation because
%	\item ...Process simulators to slow for real applications and only support outdated Architectures (GPUSim only supports up to Fermi)
%	\item ...analytical modelling usually lack the accuracy to capture exact patterns in complex communication since they are based on simplifications
%	
%\end{itemize}
\section{Goals}
This thesis has two main goals. First, develop tooling for instrumentation of GPU applications to perform dynamic global memory tracing. The instrumentation will happen at compile time using custom plugins for Clang and LLVM to transform the source code of the
original application. With this setup, traces can be generated for any application providing source code,
across different generations of Nvidia hardware. A loss in performance resulting from the traces will be tolerated.

The second goal is to use the generated traces for CTAs and kernels communicate analysis. The communication will be analysed on a qualitative level by classifying how the data is exchanged and on a quantitative level with metrics like volume, density and distance to characterize kernel and CTA interactions.


\section{Outline}
	\paragraph{Chapter 2} presents technological background information. It will introduce BSP, GPUs, LLVM, SSA and the compiler techniques used to transform the code of the original application.
	\paragraph{Chapter 3} discusses work related to this Thesis. It will explore work in the field of code instrumentation and GPU performance analysis.
	\paragraph{Chapter 4} defines communication in the context of this work and explores the analysis space.
	\paragraph{Chapter 5} presents how applications are instrumented using AST manipulation and deeper code analysis in LLVM's intermidiate representation (IR).
	\paragraph{Chapter 6} presents the set of applications that will be analysed, the different metrics are explained in detail and the results of the analysis are discussed.
	\paragraph{Chapter 7} concludes and summarizes the results of this thesis and discusses possible future directions this project could take.
