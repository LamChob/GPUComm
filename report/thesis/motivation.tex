\chapter{Motivation}
The communication of HPC applications in distributed memory systems with MPI is thoroughly researched in many publications, like  \cite{Klenk} or \cite{Riesen:2006:CP:1898699.1898795}.
With respect to the BSP modell, GPU computing has many similarities to computing in distributed memory systems. Each CTA computes independently and information is stored in a shared memory. CTAs in subsequent kernels read this data, creating patterns of communication. This resembles MPI, as each node computes locally and then communicates and synchronizes with other nodes, before the next part of computation.
Yet, intra-application communication in GPUs has not been researched, despite the fact that General Purpose GPU (GPGPU) computing is a well established part of high performance computing, as many Top500 
systems \cite{Top500} use GPUs to accelarate computation. 

In GPUs, parallel hardware continues scaling with every generation, from 512 CUDA cores in Fermi  \cite{FermiPaper} to 3840 CUDA cores in the current Pascal generation \cite{PascalPaper}. This scaling is supported by CUDA's programming model, allowing performance portable applications. With increasing potential parallelism and concurrency, communication patterns, volumes and complexity also increase or change.
An understanding of intra-application communication is fundamental for the scaling of applications to multiple
GPUs.
%HPC has massively distributed memory parallel applications, which are well researched.
%GPUs are like that, too. But not researched, yet because shared memory architecture.
%Fundamentally interesting to research behaviour, when scaling to multiple GPUs.

To create this understanding, in-depth knowledge about the communication is required. There are 
several techniques that help understand the characteristics of an application.
Spafford et al. \cite{Spafford:2012:ADS:2388996.2389110} categorized these techniques and presented several 
metrics for evaluation, including speed and accuracy. For high accuracy requirements, GPGPU simulators provide cycle-accurate simulation results for an application. However, simulating every hardware cycle in software is a time-consuming process. 
Another issue with simulation is the availability of different hardware generations. GPGPU-Sim, for example, only supports Fermi architectures. \cite{gpgpu}

Analytical modelling uses mathematical models, trying to create an approximation of the applications behaviour.
This is usually very fast, as calculations are only approximation to an applications
behaviour. Because of that, accuracy is usually lacking, especially for dynamic or data-depended applications.

MPI communication is characterized using traces. An  instrumented application  logs information on a desired metric and stores the data for later analysis. Instrumented code is faster than simulations as it runs on the actual hardware and has causes less issues with different 
hardware generations. Although slower than analytical modelling, it is more accurate since it is generated while the actual application executes, using a real dataset.
These features make instrumentation and tracing the tool of choice for this thesis.


%\begin{itemize}
%	\item GPUs are not CPUs. Different design philosophy leads to different application optimizations
%	\item GPU's concurrent hardware continues to scale
%	\item therefore optimizations of data movements, consistency etc. are required to harness compute power of new hardware generations
%	\item Complex Applications have non-trivial communication patterns in data parallel kernels, which are not researched yet.
%	\item This work is aimed to generate understanding of communication patterns to help build and optimize Tools (Mekong etc.) and Applications
%	\item Generate Traces with compiler instrumentation because
%	\item ...Process simulators to slow for real applications and only support outdated Architectures (GPUSim only supports up to Fermi)
%	\item ...analytical modelling usually lack the accuracy to capture exact patterns in complex communication since they are based on simplifications
%	
%\end{itemize}
\section{Contributions}
This thesis has two main contributions. First, create tooling for instrumentation of GPU applications to perform dynamic global memory tracing. The instrumentation will happen at compile time using custom plugins for Clang and LLVM to augment the original application. With this setup, traces can be generated for any application providing source code,
across different generations of Nvidia hardware. A loss in performance resulting from the traces will be tolerated.

The second contribution is to use the generated traces for CTA and kernel communication analysis. The communication will be analysed on a qualitative level by classifying how the data is exchanged. On a quantitative level, metrics like volume, density and distance are used to characterize kernel and CTA interactions.


\section{Outline}
	\paragraph{Chapter 2} presents technological background information. It will introduce BSP, GPUs, LLVM, SSA and the compiler techniques used to transform the code of the original application.
	\paragraph{Chapter 3} discusses work related to this Thesis. It will explore work in the field of code instrumentation and GPU performance analysis.
	\paragraph{Chapter 4} defines communication in the context of this work and explores the analysis space.
	\paragraph{Chapter 5} presents how applications are instrumented using AST manipulation and deeper code analysis in LLVM's intermidiate representation (IR).
	\paragraph{Chapter 6} presents the set of applications that will be analysed, the different metrics are explained in detail and the results of the analysis are discussed.
	\paragraph{Chapter 7} concludes and summarizes the results of this thesis and discusses possible future directions this project could take.
